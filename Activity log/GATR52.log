==========================================
SLURM_JOB_ID = 661182
SLURM_NODELIST = gnode14
SLURM_JOB_GPUS = 0,2,3
==========================================
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
/home2/prateekj/GNN-for-text-classification/main.py:179: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  g.edata['w'] = F.softmax(e)
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classifer(
  (gcn1): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
  (gcn2): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
)
[2022/4/26 12:22:21] Epoch: 1, train_loss= 4.09235, train_acc= 0.00970, val_loss= 3.92371, val_acc= 0.16233, time= 0.48483
[2022/4/26 12:22:21] Epoch: 2, train_loss= 3.70549, train_acc= 0.17248, val_loss= 3.88071, val_acc= 0.43951, time= 0.07527
[2022/4/26 12:22:21] Epoch: 3, train_loss= 3.29912, train_acc= 0.47338, val_loss= 3.83600, val_acc= 0.65391, time= 0.06990
[2022/4/26 12:22:21] Epoch: 4, train_loss= 2.88006, train_acc= 0.68430, val_loss= 3.79354, val_acc= 0.77795, time= 0.06889
[2022/4/26 12:22:21] Epoch: 5, train_loss= 2.49068, train_acc= 0.78993, val_loss= 3.75781, val_acc= 0.83461, time= 0.06535
[2022/4/26 12:22:21] Epoch: 6, train_loss= 2.17374, train_acc= 0.84946, val_loss= 3.73120, val_acc= 0.87289, time= 0.06536
[2022/4/26 12:22:21] Epoch: 7, train_loss= 1.94760, train_acc= 0.89760, val_loss= 3.71354, val_acc= 0.90046, time= 0.06418
[2022/4/26 12:22:21] Epoch: 8, train_loss= 1.80421, train_acc= 0.91614, val_loss= 3.70275, val_acc= 0.92343, time= 0.06348
[2022/4/26 12:22:21] Epoch: 9, train_loss= 1.72148, train_acc= 0.92312, val_loss= 3.69649, val_acc= 0.93109, time= 0.06341
[2022/4/26 12:22:21] Epoch: 10, train_loss= 1.67610, train_acc= 0.92329, val_loss= 3.69291, val_acc= 0.93721, time= 0.06345
[2022/4/26 12:22:21] Epoch: 11, train_loss= 1.65115, train_acc= 0.92329, val_loss= 3.69086, val_acc= 0.93721, time= 0.06335
[2022/4/26 12:22:21] Epoch: 12, train_loss= 1.63687, train_acc= 0.92346, val_loss= 3.68963, val_acc= 0.93721, time= 0.06363
[2022/4/26 12:22:21] Epoch: 13, train_loss= 1.62813, train_acc= 0.92346, val_loss= 3.68886, val_acc= 0.93721, time= 0.06350
[2022/4/26 12:22:22] Epoch: 14, train_loss= 1.62238, train_acc= 0.92346, val_loss= 3.68835, val_acc= 0.93721, time= 0.06325
[2022/4/26 12:22:22] Epoch: 15, train_loss= 1.61831, train_acc= 0.92346, val_loss= 3.68799, val_acc= 0.93721, time= 0.06341
[2022/4/26 12:22:22] Epoch: 16, train_loss= 1.61523, train_acc= 0.92346, val_loss= 3.68773, val_acc= 0.93721, time= 0.06404
[2022/4/26 12:22:22] Epoch: 17, train_loss= 1.61286, train_acc= 0.92346, val_loss= 3.68753, val_acc= 0.93721, time= 0.06368
[2022/4/26 12:22:22] Epoch: 18, train_loss= 1.61099, train_acc= 0.92482, val_loss= 3.68738, val_acc= 0.93874, time= 0.06382
[2022/4/26 12:22:22] Epoch: 19, train_loss= 1.60948, train_acc= 0.92516, val_loss= 3.68726, val_acc= 0.93874, time= 0.06389
[2022/4/26 12:22:22] Epoch: 20, train_loss= 1.60825, train_acc= 0.92533, val_loss= 3.68716, val_acc= 0.93874, time= 0.06381
[2022/4/26 12:22:22] Epoch: 21, train_loss= 1.60725, train_acc= 0.92533, val_loss= 3.68708, val_acc= 0.93874, time= 0.06373
[2022/4/26 12:22:22] Epoch: 22, train_loss= 1.60644, train_acc= 0.92533, val_loss= 3.68703, val_acc= 0.93874, time= 0.06364
[2022/4/26 12:22:22] Epoch: 23, train_loss= 1.60580, train_acc= 0.92550, val_loss= 3.68698, val_acc= 0.93874, time= 0.06483
[2022/4/26 12:22:22] Epoch: 24, train_loss= 1.60527, train_acc= 0.92567, val_loss= 3.68694, val_acc= 0.93874, time= 0.06394
[2022/4/26 12:22:22] Epoch: 25, train_loss= 1.60483, train_acc= 0.92567, val_loss= 3.68691, val_acc= 0.93874, time= 0.06402
[2022/4/26 12:22:22] Epoch: 26, train_loss= 1.60443, train_acc= 0.92567, val_loss= 3.68689, val_acc= 0.93874, time= 0.06406
[2022/4/26 12:22:22] Epoch: 27, train_loss= 1.60410, train_acc= 0.92567, val_loss= 3.68687, val_acc= 0.93874, time= 0.06365
[2022/4/26 12:22:22] Epoch: 28, train_loss= 1.60385, train_acc= 0.92584, val_loss= 3.68686, val_acc= 0.93874, time= 0.06375
[2022/4/26 12:22:22] Epoch: 29, train_loss= 1.60365, train_acc= 0.92584, val_loss= 3.68685, val_acc= 0.93874, time= 0.06387
[2022/4/26 12:22:23] Epoch: 30, train_loss= 1.60348, train_acc= 0.92584, val_loss= 3.68684, val_acc= 0.93874, time= 0.06409
[2022/4/26 12:22:23] Epoch: 31, train_loss= 1.60334, train_acc= 0.92584, val_loss= 3.68683, val_acc= 0.93874, time= 0.06415
[2022/4/26 12:22:23] Epoch: 32, train_loss= 1.60325, train_acc= 0.92584, val_loss= 3.68682, val_acc= 0.93874, time= 0.06384
[2022/4/26 12:22:23] Epoch: 33, train_loss= 1.60319, train_acc= 0.92584, val_loss= 3.68682, val_acc= 0.93874, time= 0.06403
[2022/4/26 12:22:23] Epoch: 34, train_loss= 1.60314, train_acc= 0.92584, val_loss= 3.68681, val_acc= 0.93874, time= 0.06630
[2022/4/26 12:22:23] Epoch: 35, train_loss= 1.60309, train_acc= 0.92584, val_loss= 3.68681, val_acc= 0.93874, time= 0.06407
[2022/4/26 12:22:23] Epoch: 36, train_loss= 1.60305, train_acc= 0.92584, val_loss= 3.68681, val_acc= 0.93874, time= 0.06373
[2022/4/26 12:22:23] Epoch: 37, train_loss= 1.60301, train_acc= 0.92584, val_loss= 3.68680, val_acc= 0.93874, time= 0.06868
[2022/4/26 12:22:23] Epoch: 38, train_loss= 1.60298, train_acc= 0.92584, val_loss= 3.68680, val_acc= 0.93874, time= 0.06398
[2022/4/26 12:22:23] Epoch: 39, train_loss= 1.60295, train_acc= 0.92584, val_loss= 3.68679, val_acc= 0.93874, time= 0.06384
[2022/4/26 12:22:23] Epoch: 40, train_loss= 1.60293, train_acc= 0.92584, val_loss= 3.68679, val_acc= 0.93874, time= 0.06374
[2022/4/26 12:22:23] Epoch: 41, train_loss= 1.60291, train_acc= 0.92584, val_loss= 3.68679, val_acc= 0.93874, time= 0.06393
[2022/4/26 12:22:23] Epoch: 42, train_loss= 1.60289, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06379
[2022/4/26 12:22:23] Epoch: 43, train_loss= 1.60287, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06375
[2022/4/26 12:22:23] Epoch: 44, train_loss= 1.60286, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06371
[2022/4/26 12:22:24] Epoch: 45, train_loss= 1.60284, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06381
[2022/4/26 12:22:24] Epoch: 46, train_loss= 1.60283, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06372
[2022/4/26 12:22:24] Epoch: 47, train_loss= 1.60282, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06369
[2022/4/26 12:22:24] Epoch: 48, train_loss= 1.60281, train_acc= 0.92584, val_loss= 3.68678, val_acc= 0.93874, time= 0.06386
[2022/4/26 12:22:24] Epoch: 49, train_loss= 1.60280, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06364
[2022/4/26 12:22:24] Epoch: 50, train_loss= 1.60279, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06353
[2022/4/26 12:22:24] Epoch: 51, train_loss= 1.60278, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06363
[2022/4/26 12:22:24] Epoch: 52, train_loss= 1.60278, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06380
[2022/4/26 12:22:24] Epoch: 53, train_loss= 1.60277, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06354
[2022/4/26 12:22:24] Epoch: 54, train_loss= 1.60276, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06348
[2022/4/26 12:22:24] Epoch: 55, train_loss= 1.60276, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06374
[2022/4/26 12:22:24] Epoch: 56, train_loss= 1.60275, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06390
[2022/4/26 12:22:24] Epoch: 57, train_loss= 1.60275, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06365
[2022/4/26 12:22:24] Epoch: 58, train_loss= 1.60274, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06352
[2022/4/26 12:22:24] Epoch: 59, train_loss= 1.60274, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06360
[2022/4/26 12:22:24] Epoch: 60, train_loss= 1.60273, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06367
[2022/4/26 12:22:25] Epoch: 61, train_loss= 1.60273, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06362
[2022/4/26 12:22:25] Epoch: 62, train_loss= 1.60272, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06377
[2022/4/26 12:22:25] Epoch: 63, train_loss= 1.60272, train_acc= 0.92584, val_loss= 3.68677, val_acc= 0.93874, time= 0.06391
[2022/4/26 12:22:25] Epoch: 64, train_loss= 1.60272, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06412
[2022/4/26 12:22:25] Epoch: 65, train_loss= 1.60271, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06362
[2022/4/26 12:22:25] Epoch: 66, train_loss= 1.60271, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06330
[2022/4/26 12:22:25] Epoch: 67, train_loss= 1.60271, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06355
[2022/4/26 12:22:25] Epoch: 68, train_loss= 1.60271, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06375
[2022/4/26 12:22:25] Epoch: 69, train_loss= 1.60270, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06303
[2022/4/26 12:22:25] Epoch: 70, train_loss= 1.60270, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06387
[2022/4/26 12:22:25] Epoch: 71, train_loss= 1.60270, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06338
[2022/4/26 12:22:25] Epoch: 72, train_loss= 1.60270, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06317
[2022/4/26 12:22:25] Epoch: 73, train_loss= 1.60269, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06340
[2022/4/26 12:22:25] Epoch: 74, train_loss= 1.60269, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06338
[2022/4/26 12:22:25] Epoch: 75, train_loss= 1.60269, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06356
[2022/4/26 12:22:25] Epoch: 76, train_loss= 1.60269, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06320
[2022/4/26 12:22:26] Epoch: 77, train_loss= 1.60269, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06338
[2022/4/26 12:22:26] Epoch: 78, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06330
[2022/4/26 12:22:26] Epoch: 79, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06339
[2022/4/26 12:22:26] Epoch: 80, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06302
[2022/4/26 12:22:26] Epoch: 81, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06364
[2022/4/26 12:22:26] Epoch: 82, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06338
[2022/4/26 12:22:26] Epoch: 83, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06319
[2022/4/26 12:22:26] Epoch: 84, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06334
[2022/4/26 12:22:26] Epoch: 85, train_loss= 1.60268, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06335
[2022/4/26 12:22:26] Epoch: 86, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06306
[2022/4/26 12:22:26] Epoch: 87, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06341
[2022/4/26 12:22:26] Epoch: 88, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06345
[2022/4/26 12:22:26] Epoch: 89, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06336
[2022/4/26 12:22:26] Epoch: 90, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06371
[2022/4/26 12:22:26] Epoch: 91, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06345
[2022/4/26 12:22:27] Epoch: 92, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06378
[2022/4/26 12:22:27] Epoch: 93, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06358
[2022/4/26 12:22:27] Epoch: 94, train_loss= 1.60267, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06342
[2022/4/26 12:22:27] Epoch: 95, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06352
[2022/4/26 12:22:27] Epoch: 96, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06365
[2022/4/26 12:22:27] Epoch: 97, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68676, val_acc= 0.93874, time= 0.06338
[2022/4/26 12:22:27] Epoch: 98, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68675, val_acc= 0.93874, time= 0.06337
[2022/4/26 12:22:27] Epoch: 99, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68675, val_acc= 0.93874, time= 0.06357
[2022/4/26 12:22:27] Epoch: 100, train_loss= 1.60266, train_acc= 0.92584, val_loss= 3.68675, val_acc= 0.93874, time= 0.06354
[2022/4/26 12:22:27] Optimization Finished!
[2022/4/26 12:22:27] Test set results: 
[2022/4/26 12:22:27] 	 loss= 2.93738, accuracy= 0.91745, time= 0.02929
[2022/4/26 12:22:27] Test Precision, Recall and F1-Score...
[2022/4/26 12:22:27]               precision    recall  f1-score   support
[2022/4/26 12:22:27] 
[2022/4/26 12:22:27]            0     0.9051    1.0000    0.9502       696
[2022/4/26 12:22:27]            1     1.0000    1.0000    1.0000        12
[2022/4/26 12:22:27]            2     1.0000    1.0000    1.0000         5
[2022/4/26 12:22:27]            3     0.0000    0.0000    0.0000         4
[2022/4/26 12:22:27]            4     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]            5     1.0000    1.0000    1.0000        15
[2022/4/26 12:22:27]            6     0.0000    0.0000    0.0000         3
[2022/4/26 12:22:27]            7     1.0000    1.0000    1.0000         9
[2022/4/26 12:22:27]            8     1.0000    1.0000    1.0000        75
[2022/4/26 12:22:27]            9     0.0000    0.0000    0.0000        11
[2022/4/26 12:22:27]           10     1.0000    1.0000    1.0000         9
[2022/4/26 12:22:27]           11     1.0000    1.0000    1.0000         2
[2022/4/26 12:22:27]           12     1.0000    1.0000    1.0000        10
[2022/4/26 12:22:27]           13     1.0000    1.0000    1.0000         9
[2022/4/26 12:22:27]           14     0.0000    0.0000    0.0000         5
[2022/4/26 12:22:27]           15     1.0000    1.0000    1.0000        28
[2022/4/26 12:22:27]           16     0.0000    0.0000    0.0000         6
[2022/4/26 12:22:27]           17     1.0000    1.0000    1.0000         3
[2022/4/26 12:22:27]           18     0.0000    0.0000    0.0000        11
[2022/4/26 12:22:27]           19     1.0000    1.0000    1.0000         6
[2022/4/26 12:22:27]           20     1.0000    1.0000    1.0000        25
[2022/4/26 12:22:27]           21     1.0000    1.0000    1.0000        22
[2022/4/26 12:22:27]           22     0.8863    1.0000    0.9397      1083
[2022/4/26 12:22:27]           23     1.0000    1.0000    1.0000         4
[2022/4/26 12:22:27]           24     0.0000    0.0000    0.0000        87
[2022/4/26 12:22:27]           25     1.0000    1.0000    1.0000         9
[2022/4/26 12:22:27]           26     1.0000    1.0000    1.0000        13
[2022/4/26 12:22:27]           27     0.0000    0.0000    0.0000         5
[2022/4/26 12:22:27]           28     1.0000    1.0000    1.0000        36
[2022/4/26 12:22:27]           29     1.0000    1.0000    1.0000        10
[2022/4/26 12:22:27]           30     1.0000    1.0000    1.0000        81
[2022/4/26 12:22:27]           31     1.0000    1.0000    1.0000        20
[2022/4/26 12:22:27]           32     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]           33     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]           34     1.0000    1.0000    1.0000        12
[2022/4/26 12:22:27]           35     0.0000    0.0000    0.0000         4
[2022/4/26 12:22:27]           36     0.0000    0.0000    0.0000         7
[2022/4/26 12:22:27]           37     1.0000    1.0000    1.0000         4
[2022/4/26 12:22:27]           38     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]           39     0.0000    0.0000    0.0000         2
[2022/4/26 12:22:27]           40     0.0000    0.0000    0.0000         9
[2022/4/26 12:22:27]           41     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]           42     0.0000    0.0000    0.0000         3
[2022/4/26 12:22:27]           43     1.0000    1.0000    1.0000         8
[2022/4/26 12:22:27]           44     1.0000    1.0000    1.0000       121
[2022/4/26 12:22:27]           45     1.0000    1.0000    1.0000        17
[2022/4/26 12:22:27]           46     1.0000    1.0000    1.0000        12
[2022/4/26 12:22:27]           47     0.0000    0.0000    0.0000         3
[2022/4/26 12:22:27]           48     0.0000    0.0000    0.0000         1
[2022/4/26 12:22:27]           49     0.0000    0.0000    0.0000        15
[2022/4/26 12:22:27]           50     0.0000    0.0000    0.0000        12
[2022/4/26 12:22:27]           51     0.0000    0.0000    0.0000        19
[2022/4/26 12:22:27] 
[2022/4/26 12:22:27]     accuracy                         0.9174      2568
[2022/4/26 12:22:27]    macro avg     0.5537    0.5577    0.5556      2568
[2022/4/26 12:22:27] weighted avg     0.8437    0.9174    0.8785      2568
[2022/4/26 12:22:27] 
[2022/4/26 12:22:27] Macro average Test Precision, Recall and F1-Score...
[2022/4/26 12:22:27] (0.5536792993708259, 0.5576923076923077, 0.555574364631319, None)
[2022/4/26 12:22:27] Micro average Test Precision, Recall and F1-Score...
[2022/4/26 12:22:27] (0.9174454828660437, 0.9174454828660437, 0.9174454828660437, None)
[2022/4/26 12:22:27] Embeddings:
[2022/4/26 12:22:27] Word_embeddings:57
[2022/4/26 12:22:27] Train_doc_embeddings:6532
[2022/4/26 12:22:27] Test_doc_embeddings:2568
[2022/4/26 12:22:27] Word_embeddings:
[[ 0.          0.1973494   0.         ...  0.11197099  0.12223205
   0.        ]
 [ 5.442231    9.212357   10.803845   ...  9.326008    7.4494324
   7.8688745 ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.48335314  0.9473747   0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.30852944]
 [ 0.          0.85599875  0.         ...  0.          0.
   0.        ]]
Traceback (most recent call last):
  File "/home2/prateekj/GNN-for-text-classification/main.py", line 370, in <module>
    word_vector = word_embeddings[i]
IndexError: index 57 is out of bounds for axis 0 with size 57
