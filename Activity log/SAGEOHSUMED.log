==========================================SLURM_JOB_ID = 661171SLURM_NODELIST = gnode14SLURM_JOB_GPUS = 0,2,3==========================================/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.  _warn_prf(average, modifier, msg_start, len(result))Classifer(  (gcn1): SAGEMeanConv(    (feat_drop): Dropout(p=0.5, inplace=False)  )  (gcn2): SAGEMeanConv(    (feat_drop): Dropout(p=0.5, inplace=False)  ))[2022/4/26 12:10:20] Epoch: 1, train_loss= 4.09305, train_acc= 0.08206, val_loss= 3.18676, val_acc= 0.13731, time= 0.41561[2022/4/26 12:10:20] Epoch: 2, train_loss= 3.57749, train_acc= 0.14626, val_loss= 3.14933, val_acc= 0.20597, time= 0.02023[2022/4/26 12:10:20] Epoch: 3, train_loss= 3.17364, train_acc= 0.25314, val_loss= 3.12022, val_acc= 0.29552, time= 0.02057[2022/4/26 12:10:20] Epoch: 4, train_loss= 2.86765, train_acc= 0.38451, val_loss= 3.09752, val_acc= 0.39403, time= 0.01991[2022/4/26 12:10:20] Epoch: 5, train_loss= 2.64029, train_acc= 0.50298, val_loss= 3.07983, val_acc= 0.47463, time= 0.02041[2022/4/26 12:10:20] Epoch: 6, train_loss= 2.47051, train_acc= 0.61118, val_loss= 3.06548, val_acc= 0.53731, time= 0.02021[2022/4/26 12:10:20] Epoch: 7, train_loss= 2.34057, train_acc= 0.68961, val_loss= 3.05328, val_acc= 0.60896, time= 0.01916[2022/4/26 12:10:20] Epoch: 8, train_loss= 2.23647, train_acc= 0.75347, val_loss= 3.04286, val_acc= 0.69254, time= 0.01902[2022/4/26 12:10:20] Epoch: 9, train_loss= 2.15153, train_acc= 0.80973, val_loss= 3.03401, val_acc= 0.75224, time= 0.01854[2022/4/26 12:10:20] Epoch: 10, train_loss= 2.08455, train_acc= 0.85672, val_loss= 3.02703, val_acc= 0.81493, time= 0.01860[2022/4/26 12:10:20] Epoch: 11, train_loss= 2.03507, train_acc= 0.89709, val_loss= 3.02177, val_acc= 0.84478, time= 0.01924[2022/4/26 12:10:20] Epoch: 12, train_loss= 2.00044, train_acc= 0.91794, val_loss= 3.01780, val_acc= 0.88060, time= 0.01883[2022/4/26 12:10:20] Epoch: 13, train_loss= 1.97671, train_acc= 0.93117, val_loss= 3.01473, val_acc= 0.89552, time= 0.01834[2022/4/26 12:10:20] Epoch: 14, train_loss= 1.96035, train_acc= 0.93977, val_loss= 3.01230, val_acc= 0.91045, time= 0.01848[2022/4/26 12:10:20] Epoch: 15, train_loss= 1.94877, train_acc= 0.94375, val_loss= 3.01035, val_acc= 0.92537, time= 0.01857[2022/4/26 12:10:20] Epoch: 16, train_loss= 1.94044, train_acc= 0.94772, val_loss= 3.00877, val_acc= 0.92537, time= 0.01743[2022/4/26 12:10:20] Epoch: 17, train_loss= 1.93421, train_acc= 0.95069, val_loss= 3.00742, val_acc= 0.92836, time= 0.01749[2022/4/26 12:10:20] Epoch: 18, train_loss= 1.92945, train_acc= 0.95301, val_loss= 3.00622, val_acc= 0.92836, time= 0.01729[2022/4/26 12:10:20] Epoch: 19, train_loss= 1.92543, train_acc= 0.95566, val_loss= 3.00518, val_acc= 0.93433, time= 0.01739[2022/4/26 12:10:20] Epoch: 20, train_loss= 1.92136, train_acc= 0.95797, val_loss= 3.00418, val_acc= 0.94030, time= 0.01729[2022/4/26 12:10:20] Epoch: 21, train_loss= 1.91718, train_acc= 0.96261, val_loss= 3.00312, val_acc= 0.94328, time= 0.01751[2022/4/26 12:10:20] Epoch: 22, train_loss= 1.91262, train_acc= 0.96790, val_loss= 3.00201, val_acc= 0.95522, time= 0.01728[2022/4/26 12:10:20] Epoch: 23, train_loss= 1.90726, train_acc= 0.97154, val_loss= 3.00114, val_acc= 0.95821, time= 0.01695[2022/4/26 12:10:20] Epoch: 24, train_loss= 1.90171, train_acc= 0.97684, val_loss= 3.00058, val_acc= 0.96418, time= 0.01728[2022/4/26 12:10:20] Epoch: 25, train_loss= 1.89691, train_acc= 0.97750, val_loss= 3.00034, val_acc= 0.96418, time= 0.01733[2022/4/26 12:10:20] Epoch: 26, train_loss= 1.89325, train_acc= 0.97981, val_loss= 3.00037, val_acc= 0.96418, time= 0.01705[2022/4/26 12:10:20] Epoch: 27, train_loss= 1.89100, train_acc= 0.97981, val_loss= 3.00052, val_acc= 0.96119, time= 0.01698[2022/4/26 12:10:20] Epoch: 28, train_loss= 1.88974, train_acc= 0.98015, val_loss= 3.00066, val_acc= 0.95522, time= 0.01685[2022/4/26 12:10:20] Epoch: 29, train_loss= 1.88890, train_acc= 0.98081, val_loss= 3.00068, val_acc= 0.95522, time= 0.01701[2022/4/26 12:10:20] Epoch: 30, train_loss= 1.88803, train_acc= 0.98081, val_loss= 3.00046, val_acc= 0.95522, time= 0.01702[2022/4/26 12:10:20] Epoch: 31, train_loss= 1.88689, train_acc= 0.98081, val_loss= 3.00011, val_acc= 0.95821, time= 0.01678[2022/4/26 12:10:20] Epoch: 32, train_loss= 1.88556, train_acc= 0.98213, val_loss= 2.99967, val_acc= 0.96119, time= 0.01700[2022/4/26 12:10:20] Epoch: 33, train_loss= 1.88412, train_acc= 0.98345, val_loss= 2.99920, val_acc= 0.96716, time= 0.01699[2022/4/26 12:10:20] Epoch: 34, train_loss= 1.88268, train_acc= 0.98445, val_loss= 2.99874, val_acc= 0.96716, time= 0.01676[2022/4/26 12:10:20] Epoch: 35, train_loss= 1.88139, train_acc= 0.98544, val_loss= 2.99835, val_acc= 0.97313, time= 0.01727[2022/4/26 12:10:20] Epoch: 36, train_loss= 1.88036, train_acc= 0.98544, val_loss= 2.99803, val_acc= 0.97612, time= 0.01673[2022/4/26 12:10:20] Epoch: 37, train_loss= 1.87962, train_acc= 0.98577, val_loss= 2.99779, val_acc= 0.97612, time= 0.01697[2022/4/26 12:10:21] Epoch: 38, train_loss= 1.87913, train_acc= 0.98610, val_loss= 2.99761, val_acc= 0.97910, time= 0.01699[2022/4/26 12:10:21] Epoch: 39, train_loss= 1.87879, train_acc= 0.98610, val_loss= 2.99748, val_acc= 0.97910, time= 0.01681[2022/4/26 12:10:21] Epoch: 40, train_loss= 1.87855, train_acc= 0.98610, val_loss= 2.99738, val_acc= 0.97910, time= 0.01696[2022/4/26 12:10:21] Epoch: 41, train_loss= 1.87837, train_acc= 0.98610, val_loss= 2.99730, val_acc= 0.98209, time= 0.01712[2022/4/26 12:10:21] Epoch: 42, train_loss= 1.87823, train_acc= 0.98610, val_loss= 2.99724, val_acc= 0.98209, time= 0.01715[2022/4/26 12:10:21] Epoch: 43, train_loss= 1.87811, train_acc= 0.98610, val_loss= 2.99719, val_acc= 0.98209, time= 0.01721[2022/4/26 12:10:21] Epoch: 44, train_loss= 1.87802, train_acc= 0.98610, val_loss= 2.99715, val_acc= 0.98209, time= 0.01677[2022/4/26 12:10:21] Epoch: 45, train_loss= 1.87796, train_acc= 0.98610, val_loss= 2.99711, val_acc= 0.98209, time= 0.01679[2022/4/26 12:10:21] Epoch: 46, train_loss= 1.87791, train_acc= 0.98610, val_loss= 2.99709, val_acc= 0.98209, time= 0.01701[2022/4/26 12:10:21] Epoch: 47, train_loss= 1.87788, train_acc= 0.98610, val_loss= 2.99706, val_acc= 0.98209, time= 0.01700[2022/4/26 12:10:21] Epoch: 48, train_loss= 1.87786, train_acc= 0.98610, val_loss= 2.99705, val_acc= 0.98209, time= 0.01703[2022/4/26 12:10:21] Epoch: 49, train_loss= 1.87784, train_acc= 0.98610, val_loss= 2.99703, val_acc= 0.98209, time= 0.01678[2022/4/26 12:10:21] Epoch: 50, train_loss= 1.87783, train_acc= 0.98610, val_loss= 2.99702, val_acc= 0.98209, time= 0.01668[2022/4/26 12:10:21] Epoch: 51, train_loss= 1.87783, train_acc= 0.98610, val_loss= 2.99701, val_acc= 0.98209, time= 0.01701[2022/4/26 12:10:21] Epoch: 52, train_loss= 1.87782, train_acc= 0.98610, val_loss= 2.99700, val_acc= 0.98209, time= 0.01697[2022/4/26 12:10:21] Epoch: 53, train_loss= 1.87781, train_acc= 0.98610, val_loss= 2.99699, val_acc= 0.98209, time= 0.01705[2022/4/26 12:10:21] Epoch: 54, train_loss= 1.87781, train_acc= 0.98610, val_loss= 2.99698, val_acc= 0.98209, time= 0.01676[2022/4/26 12:10:21] Epoch: 55, train_loss= 1.87780, train_acc= 0.98610, val_loss= 2.99698, val_acc= 0.98209, time= 0.01699[2022/4/26 12:10:21] Epoch: 56, train_loss= 1.87780, train_acc= 0.98610, val_loss= 2.99697, val_acc= 0.98209, time= 0.01708[2022/4/26 12:10:21] Epoch: 57, train_loss= 1.87780, train_acc= 0.98610, val_loss= 2.99697, val_acc= 0.98209, time= 0.01729[2022/4/26 12:10:21] Epoch: 58, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99696, val_acc= 0.98209, time= 0.01665[2022/4/26 12:10:21] Epoch: 59, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99696, val_acc= 0.98209, time= 0.01707[2022/4/26 12:10:21] Epoch: 60, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99695, val_acc= 0.98209, time= 0.01692[2022/4/26 12:10:21] Epoch: 61, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99695, val_acc= 0.98209, time= 0.01706[2022/4/26 12:10:21] Epoch: 62, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99695, val_acc= 0.98209, time= 0.01665[2022/4/26 12:10:21] Epoch: 63, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99695, val_acc= 0.98209, time= 0.01674[2022/4/26 12:10:21] Epoch: 64, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99694, val_acc= 0.98209, time= 0.01652[2022/4/26 12:10:21] Epoch: 65, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99694, val_acc= 0.98209, time= 0.01713[2022/4/26 12:10:21] Epoch: 66, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99694, val_acc= 0.98209, time= 0.01710[2022/4/26 12:10:21] Epoch: 67, train_loss= 1.87779, train_acc= 0.98610, val_loss= 2.99694, val_acc= 0.98209, time= 0.01681[2022/4/26 12:10:21] Epoch: 68, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99694, val_acc= 0.98209, time= 0.01711[2022/4/26 12:10:21] Epoch: 69, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01684[2022/4/26 12:10:21] Epoch: 70, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01685[2022/4/26 12:10:21] Epoch: 71, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01700[2022/4/26 12:10:21] Epoch: 72, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01682[2022/4/26 12:10:21] Epoch: 73, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01732[2022/4/26 12:10:21] Epoch: 74, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01683[2022/4/26 12:10:21] Epoch: 75, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01710[2022/4/26 12:10:21] Epoch: 76, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01705[2022/4/26 12:10:21] Epoch: 77, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99693, val_acc= 0.98209, time= 0.01689[2022/4/26 12:10:21] Epoch: 78, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01681[2022/4/26 12:10:21] Epoch: 79, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01682[2022/4/26 12:10:21] Epoch: 80, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01709[2022/4/26 12:10:21] Epoch: 81, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01698[2022/4/26 12:10:21] Epoch: 82, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01687[2022/4/26 12:10:21] Epoch: 83, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01683[2022/4/26 12:10:21] Epoch: 84, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01696[2022/4/26 12:10:21] Epoch: 85, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01703[2022/4/26 12:10:21] Epoch: 86, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01689[2022/4/26 12:10:21] Epoch: 87, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01690[2022/4/26 12:10:21] Epoch: 88, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01678[2022/4/26 12:10:21] Epoch: 89, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01685[2022/4/26 12:10:21] Epoch: 90, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01678[2022/4/26 12:10:21] Epoch: 91, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01704[2022/4/26 12:10:21] Epoch: 92, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01708[2022/4/26 12:10:21] Epoch: 93, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99692, val_acc= 0.98209, time= 0.01712[2022/4/26 12:10:21] Epoch: 94, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01705[2022/4/26 12:10:21] Epoch: 95, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01680[2022/4/26 12:10:22] Epoch: 96, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01681[2022/4/26 12:10:22] Epoch: 97, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01715[2022/4/26 12:10:22] Epoch: 98, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01692[2022/4/26 12:10:22] Epoch: 99, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01699[2022/4/26 12:10:22] Epoch: 100, train_loss= 1.87778, train_acc= 0.98610, val_loss= 2.99691, val_acc= 0.98209, time= 0.01701[2022/4/26 12:10:22] Optimization Finished![2022/4/26 12:10:22] Test set results: [2022/4/26 12:10:22] 	 loss= 1.45681, accuracy= 0.98565, time= 0.00621[2022/4/26 12:10:22] Test Precision, Recall and F1-Score...[2022/4/26 12:10:22]               precision    recall  f1-score   support[2022/4/26 12:10:22] [2022/4/26 12:10:22]            0     0.9122    1.0000    0.9541       187[2022/4/26 12:10:22]            1     1.0000    0.9924    0.9962       132[2022/4/26 12:10:22]            2     0.9368    1.0000    0.9674       178[2022/4/26 12:10:22]            3     1.0000    1.0000    1.0000       155[2022/4/26 12:10:22]            4     0.9744    1.0000    0.9870        76[2022/4/26 12:10:22]            5     0.0000    0.0000    0.0000        10[2022/4/26 12:10:22]            6     0.9655    1.0000    0.9825        28[2022/4/26 12:10:22]            7     0.9753    1.0000    0.9875        79[2022/4/26 12:10:22]            8     0.9859    1.0000    0.9929       140[2022/4/26 12:10:22]            9     1.0000    1.0000    1.0000       313[2022/4/26 12:10:22]           10     0.9983    1.0000    0.9992       600[2022/4/26 12:10:22]           11     0.9699    1.0000    0.9847       129[2022/4/26 12:10:22]           12     1.0000    1.0000    1.0000       590[2022/4/26 12:10:22]           13     0.9788    1.0000    0.9893       231[2022/4/26 12:10:22]           14     0.9714    1.0000    0.9855        34[2022/4/26 12:10:22]           15     0.0000    0.0000    0.0000        46[2022/4/26 12:10:22]           16     0.9459    1.0000    0.9722        70[2022/4/26 12:10:22]           17     0.9062    1.0000    0.9508        29[2022/4/26 12:10:22]           18     0.9904    1.0000    0.9952       103[2022/4/26 12:10:22]           19     1.0000    1.0000    1.0000       342[2022/4/26 12:10:22]           20     1.0000    1.0000    1.0000       419[2022/4/26 12:10:22]           21     0.9615    1.0000    0.9804        50[2022/4/26 12:10:22]           22     1.0000    0.9902    0.9951       102[2022/4/26 12:10:22] [2022/4/26 12:10:22]     accuracy                         0.9857      4043[2022/4/26 12:10:22]    macro avg     0.8901    0.9123    0.9009      4043[2022/4/26 12:10:22] weighted avg     0.9726    0.9857    0.9789      4043[2022/4/26 12:10:22] [2022/4/26 12:10:22] Macro average Test Precision, Recall and F1-Score...[2022/4/26 12:10:22] (0.8901199847836784, 0.9122878400372008, 0.9008663218523154, None)[2022/4/26 12:10:22] Micro average Test Precision, Recall and F1-Score...[2022/4/26 12:10:22] (0.9856542171654712, 0.9856542171654712, 0.9856542171654712, None)[2022/4/26 12:10:22] Embeddings:[2022/4/26 12:10:22] Word_embeddings:29[2022/4/26 12:10:22] Train_doc_embeddings:3357[2022/4/26 12:10:22] Test_doc_embeddings:4043[2022/4/26 12:10:22] Word_embeddings:[[0.         0.         0.         ... 0.         0.         0.45750135] [0.2637734  0.11664627 0.         ... 0.         0.         0.32918298] [0.         0.         0.         ... 0.         0.00213435 0.        ] ... [0.         0.24995735 0.         ... 0.         0.         0.        ] [0.42520708 0.         0.         ... 0.         0.         0.        ] [0.24696967 0.         0.08929157 ... 0.13342668 0.47293562 0.        ]]