==========================================
SLURM_JOB_ID = 661175
SLURM_NODELIST = gnode14
SLURM_JOB_GPUS = 0,2,3
==========================================
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'
/home2/prateekj/GNN-for-text-classification/main.py:179: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  g.edata['w'] = F.softmax(e)
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/prateekj/miniconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classifer(
  (gcn1): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
  (gcn2): MultiHeadGATLayer(
    (heads): ModuleList(
      (0): GATLayer()
      (1): GATLayer()
    )
  )
)
[2022/4/26 12:15:12] Epoch: 1, train_loss= 3.00081, train_acc= 0.01463, val_loss= 2.98115, val_acc= 0.36958, time= 0.59102
[2022/4/26 12:15:13] Epoch: 2, train_loss= 2.86624, train_acc= 0.36040, val_loss= 2.96832, val_acc= 0.37312, time= 0.21065
[2022/4/26 12:15:13] Epoch: 3, train_loss= 2.75076, train_acc= 0.36443, val_loss= 2.95687, val_acc= 0.41026, time= 0.21877
[2022/4/26 12:15:13] Epoch: 4, train_loss= 2.64704, train_acc= 0.40676, val_loss= 2.94685, val_acc= 0.45889, time= 0.20753
[2022/4/26 12:15:13] Epoch: 5, train_loss= 2.55583, train_acc= 0.46460, val_loss= 2.93867, val_acc= 0.45889, time= 0.20688
[2022/4/26 12:15:13] Epoch: 6, train_loss= 2.48099, train_acc= 0.46460, val_loss= 2.93221, val_acc= 0.45889, time= 0.20774
[2022/4/26 12:15:14] Epoch: 7, train_loss= 2.42148, train_acc= 0.46460, val_loss= 2.92726, val_acc= 0.45889, time= 0.20876
[2022/4/26 12:15:14] Epoch: 8, train_loss= 2.37548, train_acc= 0.46460, val_loss= 2.92356, val_acc= 0.45889, time= 0.20849
[2022/4/26 12:15:14] Epoch: 9, train_loss= 2.34092, train_acc= 0.46460, val_loss= 2.92102, val_acc= 0.45889, time= 0.20934
[2022/4/26 12:15:14] Epoch: 10, train_loss= 2.31707, train_acc= 0.46460, val_loss= 2.91918, val_acc= 0.45889, time= 0.20887
[2022/4/26 12:15:15] Epoch: 11, train_loss= 2.29988, train_acc= 0.46460, val_loss= 2.91780, val_acc= 0.45889, time= 0.20744
[2022/4/26 12:15:15] Epoch: 12, train_loss= 2.28699, train_acc= 0.46460, val_loss= 2.91674, val_acc= 0.45889, time= 0.21131
[2022/4/26 12:15:15] Epoch: 13, train_loss= 2.27701, train_acc= 0.46460, val_loss= 2.91590, val_acc= 0.45889, time= 0.20852
[2022/4/26 12:15:15] Epoch: 14, train_loss= 2.26905, train_acc= 0.46460, val_loss= 2.91520, val_acc= 0.45889, time= 0.20841
[2022/4/26 12:15:15] Epoch: 15, train_loss= 2.26245, train_acc= 0.46460, val_loss= 2.91467, val_acc= 0.45889, time= 0.20938
[2022/4/26 12:15:16] Epoch: 16, train_loss= 2.25737, train_acc= 0.46460, val_loss= 2.91430, val_acc= 0.45889, time= 0.20915
[2022/4/26 12:15:16] Epoch: 17, train_loss= 2.25371, train_acc= 0.46460, val_loss= 2.91404, val_acc= 0.45889, time= 0.21009
[2022/4/26 12:15:16] Epoch: 18, train_loss= 2.25119, train_acc= 0.46460, val_loss= 2.91390, val_acc= 0.45889, time= 0.20835
[2022/4/26 12:15:16] Epoch: 19, train_loss= 2.24985, train_acc= 0.46460, val_loss= 2.91383, val_acc= 0.45889, time= 0.20813
[2022/4/26 12:15:16] Epoch: 20, train_loss= 2.24924, train_acc= 0.46460, val_loss= 2.91379, val_acc= 0.45889, time= 0.20823
[2022/4/26 12:15:17] Epoch: 21, train_loss= 2.24887, train_acc= 0.46460, val_loss= 2.91374, val_acc= 0.45889, time= 0.20909
[2022/4/26 12:15:17] Epoch: 22, train_loss= 2.24841, train_acc= 0.46460, val_loss= 2.91369, val_acc= 0.45889, time= 0.20727
[2022/4/26 12:15:17] Epoch: 23, train_loss= 2.24792, train_acc= 0.46460, val_loss= 2.91364, val_acc= 0.45889, time= 0.20937
[2022/4/26 12:15:17] Epoch: 24, train_loss= 2.24743, train_acc= 0.46460, val_loss= 2.91359, val_acc= 0.45889, time= 0.20809
[2022/4/26 12:15:17] Epoch: 25, train_loss= 2.24700, train_acc= 0.46460, val_loss= 2.91355, val_acc= 0.45889, time= 0.20740
[2022/4/26 12:15:18] Epoch: 26, train_loss= 2.24665, train_acc= 0.46460, val_loss= 2.91352, val_acc= 0.45889, time= 0.20895
[2022/4/26 12:15:18] Epoch: 27, train_loss= 2.24636, train_acc= 0.46460, val_loss= 2.91350, val_acc= 0.45889, time= 0.20916
[2022/4/26 12:15:18] Epoch: 28, train_loss= 2.24613, train_acc= 0.46460, val_loss= 2.91348, val_acc= 0.45889, time= 0.20916
[2022/4/26 12:15:18] Epoch: 29, train_loss= 2.24595, train_acc= 0.46460, val_loss= 2.91346, val_acc= 0.45889, time= 0.21008
[2022/4/26 12:15:18] Epoch: 30, train_loss= 2.24581, train_acc= 0.46460, val_loss= 2.91345, val_acc= 0.45889, time= 0.21190
[2022/4/26 12:15:19] Epoch: 31, train_loss= 2.24570, train_acc= 0.46460, val_loss= 2.91344, val_acc= 0.45889, time= 0.22010
[2022/4/26 12:15:19] Epoch: 32, train_loss= 2.24563, train_acc= 0.46460, val_loss= 2.91344, val_acc= 0.45889, time= 0.20845
[2022/4/26 12:15:19] Epoch: 33, train_loss= 2.24556, train_acc= 0.46460, val_loss= 2.91343, val_acc= 0.45889, time= 0.21005
[2022/4/26 12:15:19] Epoch: 34, train_loss= 2.24551, train_acc= 0.46460, val_loss= 2.91343, val_acc= 0.45889, time= 0.20973
[2022/4/26 12:15:20] Epoch: 35, train_loss= 2.24548, train_acc= 0.46460, val_loss= 2.91343, val_acc= 0.45889, time= 0.20894
[2022/4/26 12:15:20] Epoch: 36, train_loss= 2.24547, train_acc= 0.46460, val_loss= 2.91343, val_acc= 0.45889, time= 0.20985
[2022/4/26 12:15:20] Epoch: 37, train_loss= 2.24547, train_acc= 0.46460, val_loss= 2.91342, val_acc= 0.45889, time= 0.20929
[2022/4/26 12:15:20] Epoch: 38, train_loss= 2.24543, train_acc= 0.46460, val_loss= 2.91342, val_acc= 0.45889, time= 0.20871
[2022/4/26 12:15:20] Epoch: 39, train_loss= 2.24539, train_acc= 0.46460, val_loss= 2.91342, val_acc= 0.45889, time= 0.20850
[2022/4/26 12:15:21] Epoch: 40, train_loss= 2.24538, train_acc= 0.46460, val_loss= 2.91342, val_acc= 0.45889, time= 0.20856
[2022/4/26 12:15:21] Epoch: 41, train_loss= 2.24537, train_acc= 0.46460, val_loss= 2.91342, val_acc= 0.45889, time= 0.21173
[2022/4/26 12:15:21] Epoch: 42, train_loss= 2.24537, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.22118
[2022/4/26 12:15:21] Epoch: 43, train_loss= 2.24536, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20743
[2022/4/26 12:15:21] Epoch: 44, train_loss= 2.24535, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20969
[2022/4/26 12:15:22] Epoch: 45, train_loss= 2.24535, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20912
[2022/4/26 12:15:22] Epoch: 46, train_loss= 2.24534, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20904
[2022/4/26 12:15:22] Epoch: 47, train_loss= 2.24534, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20773
[2022/4/26 12:15:22] Epoch: 48, train_loss= 2.24533, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20877
[2022/4/26 12:15:22] Epoch: 49, train_loss= 2.24533, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21073
[2022/4/26 12:15:23] Epoch: 50, train_loss= 2.24533, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21441
[2022/4/26 12:15:23] Epoch: 51, train_loss= 2.24533, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21052
[2022/4/26 12:15:23] Epoch: 52, train_loss= 2.24532, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20950
[2022/4/26 12:15:23] Epoch: 53, train_loss= 2.24532, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21050
[2022/4/26 12:15:24] Epoch: 54, train_loss= 2.24532, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21051
[2022/4/26 12:15:24] Epoch: 55, train_loss= 2.24532, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21002
[2022/4/26 12:15:24] Epoch: 56, train_loss= 2.24532, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21302
[2022/4/26 12:15:24] Epoch: 57, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.23181
[2022/4/26 12:15:24] Epoch: 58, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20810
[2022/4/26 12:15:25] Epoch: 59, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21456
[2022/4/26 12:15:25] Epoch: 60, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20971
[2022/4/26 12:15:25] Epoch: 61, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21182
[2022/4/26 12:15:25] Epoch: 62, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21006
[2022/4/26 12:15:25] Epoch: 63, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21105
[2022/4/26 12:15:26] Epoch: 64, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21130
[2022/4/26 12:15:26] Epoch: 65, train_loss= 2.24531, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21285
[2022/4/26 12:15:26] Epoch: 66, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21093
[2022/4/26 12:15:26] Epoch: 67, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.22742
[2022/4/26 12:15:27] Epoch: 68, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21193
[2022/4/26 12:15:27] Epoch: 69, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21097
[2022/4/26 12:15:27] Epoch: 70, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20983
[2022/4/26 12:15:27] Epoch: 71, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20920
[2022/4/26 12:15:27] Epoch: 72, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20859
[2022/4/26 12:15:28] Epoch: 73, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20970
[2022/4/26 12:15:28] Epoch: 74, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21033
[2022/4/26 12:15:28] Epoch: 75, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21209
[2022/4/26 12:15:28] Epoch: 76, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21128
[2022/4/26 12:15:28] Epoch: 77, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20863
[2022/4/26 12:15:29] Epoch: 78, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20948
[2022/4/26 12:15:29] Epoch: 79, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21073
[2022/4/26 12:15:29] Epoch: 80, train_loss= 2.24530, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21024
[2022/4/26 12:15:29] Epoch: 81, train_loss= 2.24529, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21036
[2022/4/26 12:15:29] Epoch: 82, train_loss= 2.24529, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.20938
[2022/4/26 12:15:30] Epoch: 83, train_loss= 2.24529, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21075
[2022/4/26 12:15:30] Epoch: 84, train_loss= 2.24529, train_acc= 0.46460, val_loss= 2.91341, val_acc= 0.45889, time= 0.21304
[2022/4/26 12:15:30] Early stopping...
[2022/4/26 12:15:30] Optimization Finished!
[2022/4/26 12:15:30] Test set results: 
[2022/4/26 12:15:30] 	 loss= 2.44121, accuracy= 0.46415, time= 0.07882
[2022/4/26 12:15:30] Test Precision, Recall and F1-Score...
[2022/4/26 12:15:30]               precision    recall  f1-score   support
[2022/4/26 12:15:30] 
[2022/4/26 12:15:30]            0     0.0898    1.0000    0.1647       398
[2022/4/26 12:15:30]            1     1.0000    1.0000    1.0000       395
[2022/4/26 12:15:30]            2     1.0000    1.0000    1.0000       364
[2022/4/26 12:15:30]            3     1.0000    1.0000    1.0000       399
[2022/4/26 12:15:30]            4     0.0000    0.0000    0.0000       310
[2022/4/26 12:15:30]            5     1.0000    1.0000    1.0000       396
[2022/4/26 12:15:30]            6     0.0000    0.0000    0.0000       396
[2022/4/26 12:15:30]            7     0.0000    0.0000    0.0000       394
[2022/4/26 12:15:30]            8     0.0000    0.0000    0.0000       392
[2022/4/26 12:15:30]            9     0.0000    0.0000    0.0000       251
[2022/4/26 12:15:30]           10     0.0000    0.0000    0.0000       396
[2022/4/26 12:15:30]           11     1.0000    1.0000    1.0000       376
[2022/4/26 12:15:30]           12     1.0000    1.0000    1.0000       389
[2022/4/26 12:15:30]           13     1.0000    1.0000    1.0000       394
[2022/4/26 12:15:30]           14     0.0000    0.0000    0.0000       398
[2022/4/26 12:15:30]           15     0.0000    0.0000    0.0000       393
[2022/4/26 12:15:30]           16     0.0000    0.0000    0.0000       319
[2022/4/26 12:15:30]           17     0.0000    0.0000    0.0000       397
[2022/4/26 12:15:30]           18     0.0000    0.0000    0.0000       390
[2022/4/26 12:15:30]           19     1.0000    1.0000    1.0000       385
[2022/4/26 12:15:30] 
[2022/4/26 12:15:30]     accuracy                         0.4642      7532
[2022/4/26 12:15:30]    macro avg     0.4045    0.4500    0.4082      7532
[2022/4/26 12:15:30] weighted avg     0.4161    0.4642    0.4200      7532
[2022/4/26 12:15:30] 
[2022/4/26 12:15:30] Macro average Test Precision, Recall and F1-Score...
[2022/4/26 12:15:30] (0.4044880469102391, 0.45, 0.4082367549668874, None)
[2022/4/26 12:15:30] Micro average Test Precision, Recall and F1-Score...
[2022/4/26 12:15:30] (0.4641529474243229, 0.4641529474243229, 0.4641529474243229, None)
[2022/4/26 12:15:30] Embeddings:
[2022/4/26 12:15:30] Word_embeddings:39
[2022/4/26 12:15:30] Train_doc_embeddings:11314
[2022/4/26 12:15:30] Test_doc_embeddings:7532
[2022/4/26 12:15:30] Word_embeddings:
[[0.01658046 0.0441229  0.04633915 ... 0.04045105 0.05465487 0.07022607]
 [0.50775933 0.         0.60627604 ... 0.         4.3872576  2.6042562 ]
 [7.8346567  0.         5.7375517  ... 8.705026   5.197171   6.9205346 ]
 ...
 [0.         0.         1.8561549  ... 2.8114934  3.60651    2.9649413 ]
 [0.         0.         0.         ... 2.292324   3.3321342  5.730316  ]
 [0.         0.         0.         ... 4.9217825  2.9746554  2.3114328 ]]
Traceback (most recent call last):
  File "/home2/prateekj/GNN-for-text-classification/main.py", line 370, in <module>
    word_vector = word_embeddings[i]
IndexError: index 39 is out of bounds for axis 0 with size 39
